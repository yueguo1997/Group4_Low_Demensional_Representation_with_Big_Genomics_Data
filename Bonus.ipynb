{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOW2Hgfpzx9vRVokW2aAUXv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yueguo1997/Group4_Low_Demensional_Representation_with_Big_Genomics_Data/blob/main/Bonus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rDp_dq7OSRh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Package"
      ],
      "metadata": {
        "id": "oBEUfZcROXw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "devcVrwqOZbZ",
        "outputId": "0e0ab690-5a25-4ef6-a75d-55289f81b39f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "cur_path = \"/content/drive/MyDrive/Bigdataproject/\"\n",
        "os.chdir(cur_path)\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yr3a149GOaNT",
        "outputId": "84fb0a32-3883-4cb2-dbcd-175e2df46f94"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Bigdataproject\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install pyspark[sql]\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li8RHIhIOdpt",
        "outputId": "b9c9e691-9cbf-4a62-adcc-6a3b07e180cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.9/dist-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark[sql] in /usr/local/lib/python3.9/dist-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.9/dist-packages (from pyspark[sql]) (0.10.9.5)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.9/dist-packages (from pyspark[sql]) (1.5.3)\n",
            "Requirement already satisfied: pyarrow>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pyspark[sql]) (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.5->pyspark[sql]) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.5->pyspark[sql]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.5->pyspark[sql]) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.5->pyspark[sql]) (1.16.0)\n",
            "openjdk-8-jdk-headless is already the newest version (8u362-ga-0ubuntu1~20.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark as spark\n",
        "import time\n",
        "from operator import add\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.linalg import Vectors"
      ],
      "metadata": {
        "id": "XMNtNVzKOdrx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "TUJ5WzjSOdtl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "On2J51ZhOdvq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read data"
      ],
      "metadata": {
        "id": "3OWbQ1WuQQkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set spark session\n",
        "ss = (SparkSession\n",
        "  .builder\n",
        "  .master(\"local[5]\")\n",
        "  .appName(\"fqproject\")\n",
        "  .getOrCreate())"
      ],
      "metadata": {
        "id": "mw4zt_irOdyC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set spark session\n",
        "sc = (spark\n",
        "  .SparkContext\n",
        "  .getOrCreate(spark\n",
        "    .SparkConf()\n",
        "    .setAppName(\"fqproject\")\n",
        "    .setMaster('local[*]')\n",
        "    .set('spark.executor.memory', '4G')\n",
        "    .set('spark.driver.memory', '4G')\n",
        "    .set('spark.driver.maxResultSize', '4G')))"
      ],
      "metadata": {
        "id": "u3wFtHrEOdzT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read reads.fq and extract the second line of every four lines.\n",
        "reads = (sc\n",
        "  .textFile(cur_path + \"data/reads.fq\")\n",
        "  .zipWithIndex()\n",
        "  .filter(lambda x: (x[1]+1)%2==0 and (x[1]+1)%4!=0)\n",
        "  .map(lambda x: x[0]))"
      ],
      "metadata": {
        "id": "1j6bzyeBR_KY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read reference\n",
        "reference = (ss\n",
        "  .read\n",
        "  .csv(\n",
        "      cur_path + \"data/reference_chr21_20000000_20050000.fa\",\n",
        "      inferSchema=True,\n",
        "      header=True))"
      ],
      "metadata": {
        "id": "fr9UMYRAR_Mc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cmv1snpjR_OR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# kmer function"
      ],
      "metadata": {
        "id": "u_MxGZROSO_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_kmers(s, k):\n",
        "    return [s[i:i+k] for i in range(len(s)-k+1)]"
      ],
      "metadata": {
        "id": "mQj25y4WSYng"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_kmer_features_old(read,name_list):\n",
        "  m = []\n",
        "  kmers_list = list(kmers)\n",
        "  for i in range(len(kmers_list)):\n",
        "    if kmers_list[i] in read:\n",
        "      m.append(i)\n",
        "  v = [1.0 for i in range(len(m))]\n",
        "  return (name_list[read],Vectors.sparse(len(kmers_list),m,v))"
      ],
      "metadata": {
        "id": "pT5wm_IXVjSO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# feature function"
      ],
      "metadata": {
        "id": "amtq5UIWVnEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r_list = reads.collect()\n",
        "read_name = {r_list[i]: 'reads' + str(i) for i in range(len(r_list))}\n"
      ],
      "metadata": {
        "id": "2IugKVx5ZGRn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_rdd = reference.select(\"sequence\").rdd.map(lambda x: x['sequence'])\n",
        "reference_list = reference_rdd.collect()\n",
        "start_rdd = reference.select(\"start\").rdd.map(lambda x: x['start'])\n",
        "end_rdd = reference.select(\"end\").rdd.map(lambda x: x['end'])\n",
        "s_list = start_rdd.collect()\n",
        "e_list = end_rdd.collect()\n",
        "reference_name = ['bin' + str(s_list[i]) + '_' +  str(e_list[i]) for i in range(len(s_list))]\n",
        "reference_name_d = {reference_list[i]: reference_name[i] for i in range(len(reference_list))}"
      ],
      "metadata": {
        "id": "aSmJ3FX8ZGT7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jq_soeZ5ZGVy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DKGcfzqeXk9V"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MinHash"
      ],
      "metadata": {
        "id": "SJca9kZ-dkrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define minhash\n",
        "def minhash(hashtable: list, entry: tuple) -> tuple:\n",
        "  ans = []\n",
        "  for ls in hashtable:\n",
        "    for i in ls:\n",
        "      if entry[1][ls[i]]==1:\n",
        "        ans.append(i)\n",
        "        break\n",
        "  return (entry[0], ans)"
      ],
      "metadata": {
        "id": "iQLGztp1YiKD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def minhash_table(n: int) -> list:\n",
        "  ans = []\n",
        "  for i in range(n):\n",
        "    ans.append(random.sample(range(len(kmers)), len(kmers)))\n",
        "  return ans"
      ],
      "metadata": {
        "id": "wBn9CF4hYiMn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(114514)"
      ],
      "metadata": {
        "id": "K2No34d-YiOr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zTzjUGsKYiQy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# generate hashes and write"
      ],
      "metadata": {
        "id": "u8wxOLMQe1JA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the range\n",
        "kmer_range = [5,10,15,20]\n",
        "permunate = [500,1000,2000]"
      ],
      "metadata": {
        "id": "_6oBsiQDe4Se"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop every settings\n",
        "for k_n in kmer_range:\n",
        "  for p_n in permunate:\n",
        "    # generate kmers\n",
        "    reads_kmers = reads.map(lambda x: extract_kmers(x,k_n))\n",
        "    reference_kmers = (reference.select('sequence').rdd.flatMap(lambda x: x).map(lambda x: extract_kmers(x,k_n)))\n",
        "    kmers = set(reads_kmers.union(reference_kmers).flatMap(lambda x: x) .distinct().collect())\n",
        "\n",
        "    # use kemers to check the read and reference features\n",
        "    reads_features = reads.map(lambda x: check_kmer_features_old(x,read_name))\n",
        "    reference_features = reference_rdd.map(lambda x: check_kmer_features_old(x,reference_name_d))\n",
        "\n",
        "    print(k_n,p_n,\"features generation finished\")\n",
        "\n",
        "    # generate hash table\n",
        "    hashtable = minhash_table(p_n)\n",
        "    print(k_n,p_n,\"hash table finished\")\n",
        "\n",
        "    #generate read hash features\n",
        "    read_sig = reads_features.map(lambda x: minhash(hashtable, x))\n",
        "    start = time.time()\n",
        "    print(time.localtime(start))\n",
        "    read_sig = reads_features.map(lambda x: minhash(hashtable, x))\n",
        "    print('read hash generation time', time.time()-start)\n",
        "\n",
        "    #generate ref hash features\n",
        "    start = time.time()\n",
        "    print(time.localtime(start))\n",
        "    ref_sig = reference_features.map(lambda x: minhash(hashtable, x))\n",
        "    print('ref hash generation time', time.time()-start)\n",
        "    \n",
        "    read_l = read_sig.collect()\n",
        "    ref_l = ref_sig.collect()\n",
        "\n",
        "    print('hash list generated')\n",
        "\n",
        "    # generate similarities\n",
        "\n",
        "    final_ref = []\n",
        "    final_score = []\n",
        "    for i in read_l:\n",
        "      max_score = 0\n",
        "      max_ref = ''\n",
        "      for j in ref_l:\n",
        "        score = len([1 for e in range(p_n) if i[1][e] == j[1][e]])/p_n\n",
        "        if score > max_score:\n",
        "          max_ref = j[0]\n",
        "          max_score = score\n",
        "      final_ref.append(max_ref)\n",
        "      final_score.append(max_score)\n",
        "    final_read = [i[0] for i in read_l]\n",
        "    print(k_n,p_n,\"similarity finished\")\n",
        "    final_df = pd.DataFrame({'read': final_read,'reference': final_ref,'score': final_score})\n",
        "    file_name = 'final_result_k' + str(k_n) + '_p' + str(p_n) + '.csv'\n",
        "    final_df.to_csv(cur_path + \"data/\" + file_name, header=True, index=None, sep=',')\n",
        "    print(k_n,p_n,\"csv file finished\")\n",
        "      \n",
        "\n",
        "    print('-----------------------------------------------------------')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLo24DZDfl8B",
        "outputId": "7f6c8462-6d2b-4ee2-dba6-e6fccedcb35e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 500 features generation finished\n",
            "5 500 hash table finished\n",
            "time.struct_time(tm_year=2023, tm_mon=4, tm_mday=11, tm_hour=23, tm_min=51, tm_sec=8, tm_wday=1, tm_yday=101, tm_isdst=0)\n",
            "read hash generation time 5.6743621826171875e-05\n",
            "time.struct_time(tm_year=2023, tm_mon=4, tm_mday=11, tm_hour=23, tm_min=51, tm_sec=8, tm_wday=1, tm_yday=101, tm_isdst=0)\n",
            "ref hash generation time 8.320808410644531e-05\n",
            "hash list generated\n",
            "5 500 similarity finished\n",
            "5 500 csv file finished\n",
            "-----------------------------------------------------------\n",
            "5 1000 features generation finished\n",
            "5 1000 hash table finished\n",
            "time.struct_time(tm_year=2023, tm_mon=4, tm_mday=11, tm_hour=23, tm_min=54, tm_sec=32, tm_wday=1, tm_yday=101, tm_isdst=0)\n",
            "read hash generation time 7.82012939453125e-05\n",
            "time.struct_time(tm_year=2023, tm_mon=4, tm_mday=11, tm_hour=23, tm_min=54, tm_sec=32, tm_wday=1, tm_yday=101, tm_isdst=0)\n",
            "ref hash generation time 3.3855438232421875e-05\n",
            "hash list generated\n",
            "5 1000 similarity finished\n",
            "5 1000 csv file finished\n",
            "-----------------------------------------------------------\n",
            "5 2000 features generation finished\n",
            "5 2000 hash table finished\n",
            "time.struct_time(tm_year=2023, tm_mon=4, tm_mday=12, tm_hour=0, tm_min=1, tm_sec=13, tm_wday=2, tm_yday=102, tm_isdst=0)\n",
            "read hash generation time 9.250640869140625e-05\n",
            "time.struct_time(tm_year=2023, tm_mon=4, tm_mday=12, tm_hour=0, tm_min=1, tm_sec=13, tm_wday=2, tm_yday=102, tm_isdst=0)\n",
            "ref hash generation time 4.458427429199219e-05\n",
            "hash list generated\n",
            "5 2000 similarity finished\n",
            "5 2000 csv file finished\n",
            "-----------------------------------------------------------\n",
            "10 500 features generation finished\n",
            "10 500 hash table finished\n",
            "time.struct_time(tm_year=2023, tm_mon=4, tm_mday=12, tm_hour=0, tm_min=15, tm_sec=2, tm_wday=2, tm_yday=102, tm_isdst=0)\n",
            "read hash generation time 7.367134094238281e-05\n",
            "time.struct_time(tm_year=2023, tm_mon=4, tm_mday=12, tm_hour=0, tm_min=15, tm_sec=2, tm_wday=2, tm_yday=102, tm_isdst=0)\n",
            "ref hash generation time 4.1484832763671875e-05\n",
            "hash list generated\n",
            "10 500 similarity finished\n",
            "10 500 csv file finished\n",
            "-----------------------------------------------------------\n",
            "10 1000 features generation finished\n",
            "10 1000 hash table finished\n",
            "time.struct_time(tm_year=2023, tm_mon=4, tm_mday=12, tm_hour=2, tm_min=0, tm_sec=6, tm_wday=2, tm_yday=102, tm_isdst=0)\n",
            "read hash generation time 6.604194641113281e-05\n",
            "time.struct_time(tm_year=2023, tm_mon=4, tm_mday=12, tm_hour=2, tm_min=0, tm_sec=6, tm_wday=2, tm_yday=102, tm_isdst=0)\n",
            "ref hash generation time 4.124641418457031e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# because of the long runing time. I split this operation in two times. The notebook may not display all the result"
      ],
      "metadata": {
        "id": "USr88fLOOr_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BsgdQwSZtwy0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}